import gin.tf.external_configurables

SC2Env.step_mul = 8
SC2Env.spatial_dim = 16

tf.train.AdamOptimizer.learning_rate = 0.0005

AdvantageActorCriticAgent.model_fn = @build_fully_conv
AdvantageActorCriticAgent.policy_cls = @SC2MultiPolicy

AdvantageActorCriticAgent.batch_sz = 2000
AdvantageActorCriticAgent.discount = 0.95
AdvantageActorCriticAgent.gae_lambda = 0.0
AdvantageActorCriticAgent.clip_rewards = 0.0
AdvantageActorCriticAgent.clip_grads_norm = 100.0
AdvantageActorCriticAgent.bootstrap_terminals = False
AdvantageActorCriticAgent.normalize_advantages = True
AdvantageActorCriticAgent.optimizer = @tf.train.AdamOptimizer()

AdvantageActorCriticAgent.value_coef = 0.5
AdvantageActorCriticAgent.entropy_coef = 0.001

ProximalPolicyOptimizationAgent.value_coef = 0.5
ProximalPolicyOptimizationAgent.entropy_coef = 0.01
ProximalPolicyOptimizationAgent.minibatch_sz = 250