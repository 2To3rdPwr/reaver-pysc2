import gin.tf.external_configurables

SC2Env.step_mul = 8
SC2Env.spatial_dim = 16

tf.train.AdamOptimizer.learning_rate = 0.0005

ActorCriticAgent.model_fn = @build_fully_conv
ActorCriticAgent.policy_cls = @SC2MultiPolicy

ActorCriticAgent.batch_sz = 2000
ActorCriticAgent.discount = 0.95
ActorCriticAgent.gae_lambda = 0.0
ActorCriticAgent.clip_rewards = 0.0
ActorCriticAgent.clip_grads_norm = 100.0
ActorCriticAgent.bootstrap_terminals = False
ActorCriticAgent.normalize_advantages = True
ActorCriticAgent.optimizer = @tf.train.AdamOptimizer()

AdvantageActorCriticAgent.value_coef = 0.5
AdvantageActorCriticAgent.entropy_coef = 0.001

ProximalPolicyOptimizationAgent.value_coef = 0.5
ProximalPolicyOptimizationAgent.entropy_coef = 0.01
ProximalPolicyOptimizationAgent.minibatch_sz = 250